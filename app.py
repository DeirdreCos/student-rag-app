import os
import re
import streamlit as st
import streamlit.components.v1 as components

from pypdf import PdfReader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# â”€â”€ 1. App Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ğŸ“š Student RAG MVP", layout="wide")
st.title("ğŸ“– Research Assistant with Inline PDF Viewer")

# â”€â”€ 2. OpenAI Key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Make sure youâ€™ve added OPENAI_API_KEY in Settings â†’ Secrets on Streamlit Cloud
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# â”€â”€ 3. PDF Uploader & Chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded_files = st.file_uploader(
    "Upload PDF(s) from your reading list",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    for pdf in uploaded_files:
        # 3.1 Save upload to disk
        with open(pdf.name, "wb") as f:
            f.write(pdf.getbuffer())

        # 3.2 Load pages with pypdf
        reader = PdfReader(pdf.name)
        pages = []
        for i, page in enumerate(reader.pages, start=1):
            text = page.extract_text() or ""
            pages.append(
                Document(
                    page_content=text,
                    metadata={"source": pdf.name, "page_number": i},
                )
            )

        # 3.3 Split pages into chunks
        chunks = splitter.split_documents(pages)
        for c in chunks:
            docs.append(c)

    st.success(f"Embedded {len(docs)} chunks from {len(uploaded_files)} files")

    # â”€â”€ 4. Build Vector Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    embeddings = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(
        documents=docs,
        embedding=embeddings
    )

    # â”€â”€ 5. Search UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    query = st.text_input("ğŸ” Enter a keyword/phrase or Boolean query")
    k     = st.slider("Top k matches", 1, 20, 5)

    if query:
        results = vectordb.similarity_search(query, k=k)
        st.markdown(f"### ğŸ” Top {len(results)} passages for: *{query}*")

        for idx, doc in enumerate(results, start=1):
            src = doc.metadata["source"]
            pg  = doc.metadata["page_number"]

            # build ~150â€‘word snippet around the query match
            text  = re.sub(r"\s+", " ", doc.page_content)
            words = text.split()
            pos   = next(
                (i for i,w in enumerate(words)
                 if re.search(re.escape(query), w, re.IGNORECASE)),
                len(words)//2
            )
            start, end = max(0, pos-75), min(len(words), pos+75)
            snippet = " ".join(words[start:end])
            snippet = re.sub(
                re.escape(query),
                lambda m: f"**{m.group(0)}**",
                snippet,
                flags=re.IGNORECASE
            )

            # display in expander with inline PDF page
            with st.expander(f"{idx}. {src}  (p.{pg})"):
                st.write(snippet + " â€¦")
                pdf_url = f"{src}#page={pg}"
                components.iframe(pdf_url, width=700, height=500)
